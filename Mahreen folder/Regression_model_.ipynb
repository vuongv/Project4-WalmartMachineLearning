{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Mwn1pe_mQCW6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Import and read the walmart.csv file\n",
        "application_df = pd.read_csv(\"walmart.csv\")\n",
        "\n",
        "# Drop irrelevant columns (Store and Date)\n",
        "application_df.drop(columns=['Store', 'Date'], inplace=True)\n",
        "\n",
        "# Convert 'Holiday_Flag' to a binary target variable (1 if it's a holiday, 0 if it's not)\n",
        "application_df['Is_Holiday'] = application_df['Holiday_Flag']\n",
        "\n",
        "# Drop the 'Holiday_Flag' column since we have extracted the target variable\n",
        "application_df.drop(columns=['Holiday_Flag'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Separate the features and the target variable\n",
        "X = application_df.drop(columns=['Is_Holiday'])\n",
        "y = application_df['Is_Holiday']\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "SV2SO8xcRnGb"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Create and train the logistic regression model\n",
        "logistic_model = LogisticRegression()\n",
        "logistic_model.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "-BbI-mDsRozh",
        "outputId": "aa1d92f0-eaf3-4b1d-d291-6ad1aa31e638"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression()"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = logistic_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "classification_rep = classification_report(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_rep)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4RfnlOcbRs-9",
        "outputId": "0480306a-74ee-4cf8-db86-207f2648a0d1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9191919191919192\n",
            "Confusion Matrix:\n",
            "[[1183    0]\n",
            " [ 104    0]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      1.00      0.96      1183\n",
            "           1       0.00      0.00      0.00       104\n",
            "\n",
            "    accuracy                           0.92      1287\n",
            "   macro avg       0.46      0.50      0.48      1287\n",
            "weighted avg       0.84      0.92      0.88      1287\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#The logistic regression model was trained and evaluated on the Walmart sales data, with the target variable being whether a day is a holiday or not (binary classification: 1 if it's a holiday, 0 if it's not). The results of the evaluation are as follows:\n",
        "##Accuracy: 0.9192\n",
        "#The accuracy of the model is approximately 91.92%, which means that it correctly predicts whether a day is a holiday or not for about 91.92% of the test samples.\n",
        "#Confusion Matrix: [[1183    0] [ 104    0]]\n",
        "#The confusion matrix shows the counts of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN). In this case, the model correctly predicted 1183 non-holiday days (class 0) and 0 holiday days (class 1) as true negatives. However, it incorrectly predicted 104 holiday days as non-holiday (false negatives).\n",
        "#The model performs well in predicting non-holiday days (class 0) but fails to predict any holiday days (class 1). The low recall and F1-score for class 1 indicate that the model has difficulties identifying holiday days correctly, possibly due to the class imbalance and lack of sufficient information to distinguish holiday days from non-holiday days."
      ],
      "metadata": {
        "id": "wfKvAVxpV21y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8YpnjDvNWed_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# More robust and useful predictive model.\n",
        "#To build a more useful predictive model for imbalanced datasets, we used techniques to address the class imbalance issue and optimize the model's hyperparameters.\n",
        "#We used the SMOTE technique to oversample the minority class (successful instances) to balance the dataset.\n",
        "#Hyperparameter tuning and feature scaling can significantly impact model performance, so experimenting with different combinations can lead to better results"
      ],
      "metadata": {
        "id": "e37KP3oXaJPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We used GridSearchCV which is a technique that performs an exhaustive search over a specified parameter grid to find the best hyperparameter values for a given model. It systematically trains and evaluates the model with all possible combinations of hyperparameters provided in the grid and selects the combination that yields the best performance according to a specified evaluation metric (e.g., accuracy, F1-score, etc.)."
      ],
      "metadata": {
        "id": "uuloGfFje79u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from imblearn.over_sampling import SMOTE"
      ],
      "metadata": {
        "id": "YNpGrFePaLzN"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Walmart sales data\n",
        "application_df = pd.read_csv(\"walmart.csv\")"
      ],
      "metadata": {
        "id": "zCOCAsE9agCx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert 'Date' column to datetime type if it's not already\n",
        "application_df['Date'] = pd.to_datetime(application_df['Date'], dayfirst=True)  # Specify dayfirst=True for DD/MM/YYYY format\n"
      ],
      "metadata": {
        "id": "mDrfhzCaamuZ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Engineering - Create additional features if applicable\n",
        "# For example, you can calculate lag features for 'Weekly_Sales' to capture the previous week's sales.\n",
        "application_df['Previous_Week_Sales'] = application_df['Weekly_Sales'].shift(1)\n"
      ],
      "metadata": {
        "id": "lTa_MH6VarGV"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the data\n",
        "X = application_df[['Holiday_Flag', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'Previous_Week_Sales']]\n",
        "threshold_value = 1500000  # Set the threshold value to separate the classes\n",
        "y = (application_df['Weekly_Sales'] > threshold_value).astype(int)"
      ],
      "metadata": {
        "id": "u4qVc45JcCjv"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle missing values (NaN) using SimpleImputer\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X = imputer.fit_transform(X)"
      ],
      "metadata": {
        "id": "C2UUrsRucFJI"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "cksRjuyycNgk"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle class imbalance using SMOTE (Synthetic Minority Over-sampling Technique)\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "CDqfVSm8cQ7C"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_resampled)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "4poReQ8wcU6O"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter Tuning - Experiment with different hyperparameters\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [None, 5, 10, 20],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['auto', 'sqrt']\n",
        "}\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
        "grid_search.fit(X_train_scaled, y_train_resampled)\n",
        "best_model = grid_search.best_estimator_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lY60R2XfcddZ",
        "outputId": "708efe09-ff24-41a2-e090-7f9eea2e4876"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 216 candidates, totalling 1080 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the test set using the best model\n",
        "y_pred = best_model.predict(X_test_scaled)"
      ],
      "metadata": {
        "id": "UKtjD7BachEG"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the accuracy to evaluate the model's performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7yC22pxcrNm",
        "outputId": "3c93faea-6300-41c9-bad1-30bd499baa63"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9393939393939394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the confusion matrix and classification report\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BpUrnU8FcuS_",
        "outputId": "de3f50ee-0c3a-4085-a23c-ea3cf3428f28"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[959  37]\n",
            " [ 41 250]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_report = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\")\n",
        "print(class_report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIfxtwKIcxWl",
        "outputId": "47f1b2d1-0339-48a4-d9dd-29fee5b6bf8e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.96      0.96       996\n",
            "           1       0.87      0.86      0.87       291\n",
            "\n",
            "    accuracy                           0.94      1287\n",
            "   macro avg       0.92      0.91      0.91      1287\n",
            "weighted avg       0.94      0.94      0.94      1287\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#The classification report suggests that the model performs well in predicting both classes, with good precision and recall values.\n",
        "#The feature importance analysis shows the relative importance of each feature in making predictions.\n",
        "#'Unemployment' is the most important feature, with a high importance value of 0.747. It plays a crucial role in determining whether the weekly sales will exceed the threshold or not.\n",
        "# Features like 'Fuel_Price', 'CPI', and 'Temperature' also contribute to the model's decision-making, though to a lesser extent.\n",
        "#'Weekly_Sales' itself has a very low importance value, indicating that it may not be a strong predictor for classifying the sales into the two categories."
      ],
      "metadata": {
        "id": "firsqkUPjaaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the feature importances\n",
        "feature_importance = best_model.feature_importances_\n",
        "print(\"Feature Importance:\")\n",
        "print(list(zip(application_df.columns[2:], feature_importance)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90NkyCJFc6RH",
        "outputId": "2a2f62b6-de2d-4c59-f74b-af46883bdbea"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importance:\n",
            "[('Weekly_Sales', 0.020817000474654193), ('Holiday_Flag', 0.042408509054831046), ('Temperature', 0.045910081775601845), ('Fuel_Price', 0.07322830381295282), ('CPI', 0.07105031352314232), ('Unemployment', 0.7465857913588178)]\n"
          ]
        }
      ]
    }
  ]
}